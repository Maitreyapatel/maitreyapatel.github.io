<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Maitreya Patel</title> <meta name="author" content="Maitreya Patel"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="Maitreya, GenerativeAI, Multimodal, Diffusion/Flow Modeling"/> <meta property="og:site_name" content="Maitreya Patel"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Maitreya Patel | About"/> <meta property="og:url" content="https://maitreyapatel.com/"/> <meta property="og:description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="About"/> <meta name="twitter:description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="twitter:site" content="@patelmaitreya"/> <meta name="twitter:creator" content="@patelmaitreya"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>‚öõÔ∏è</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maitreyapatel.com/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">Community Services</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Maitreya</span> Patel </h1> <p class="desc"><a href="#">Ph.D. Student</a>, School of Computing &amp; AI, Arizona State University.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_photo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_photo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_photo-1400.webp"></source> <img src="/assets/img/profile_photo.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="profile_photo.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>I am a senior Ph.D. student at Arizona State University (ASU). I am working alongside <a href="https://yezhouyang.engineering.asu.edu" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> and <a href="https://cogintlab-asu.github.io/" target="_blank" rel="noopener noreferrer">Chitta Baral</a>. I closely collaborate with <a href="https://www.tejasgokhale.com" target="_blank" rel="noopener noreferrer">Tejas Gokhale</a> and <a href="https://sites.google.com/asu.edu/changhoonkim" target="_blank" rel="noopener noreferrer">Changhoon Kim</a>.</p> <p>My research focuses on the <code class="language-plaintext highlighter-rouge">theoretical foundations of visual generative models</code> and their applications in conditional sampling, including image/video editing, inverse problems, and personalization. I am also interested in representation learning, large-scale multimodal foundational models, and <code class="language-plaintext highlighter-rouge">inference-time steering</code> to enhance the controllability and reliability of generative models. <strong>I believe true World Models must be generalizable, efficient, controllable, responsible, and grounded in physical laws.</strong></p> <p>üöÄ üöÄ Alongside my research, I am writing <a href="https://the-stochastic-journey.github.io/" target="_blank" rel="noopener noreferrer">The Stochastic Journey</a> ‚Äî a blog series that delves into the mathematical foundations of generative models, tracing their roots in stochastic calculus, probability theory, and differential equations.</p> <p>I have extensive experience in developing large-scale <span style="color: #4CAF50;">text-to-image diffusion/flow</span> and <span style="color: #2196F3;">unified multimodal models</span>, working across the full development lifecycle including <span style="color: #4CAF50;">mathematical foundations</span>, <span style="color: #4CAF50;">model architecture design</span>, <span style="color: #2196F3;">pre-training</span> and <span style="color: #2196F3;">RL alignment</span>. I focus on advancing both the fundamental understanding and building <span style="color: #4CAF50;">end-to-end systems</span> that push the boundaries of what‚Äôs possible with <span style="color: #4CAF50;">diffusion</span> and <span style="color: #2196F3;">multimodal models</span>. </p> <p style="color:red;">Note: I am currently not taking on any new students for supervision. However, if you have a well-defined research proposal and can clearly articulate how we might collaborate, I welcome you to reach out.</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%61%69%74%72%65%79%61.%70%61%74%65%6C@%61%73%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=z--mlKgAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/maitreyapatel" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/maitreya-patel-a37a16139" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/patelmaitreya" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep 18, 2025</th> <td> <a href="#">EraseFlow</a> accepted at NeurIPS‚Äô25 as <code class="language-plaintext highlighter-rouge">Spotlight</code>. Stay tuned for more details! <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 25, 2025</th> <td> üöÄüöÄ <a href="https://flowchef.github.io/" target="_blank" rel="noopener noreferrer">FlowChef</a> and <a href="https://refedit.vercel.app/" target="_blank" rel="noopener noreferrer">RefEdit</a> are accepted at <code class="language-plaintext highlighter-rouge">ICCV 2025</code>! We‚Äôll also host a tutorial. See you at Hawaii! </td> </tr> <tr> <th scope="row">Jun 5, 2025</th> <td> üìù Released <strong>RefEdit</strong> - a referring expression based image editing framework. Check out our <a href="https://arxiv.org/abs/2506.03448" target="_blank" rel="noopener noreferrer">paper</a> and <a href="https://refedit.vercel.app/" target="_blank" rel="noopener noreferrer">page</a>! ‚ú® </td> </tr> <tr> <th scope="row">May 19, 2025</th> <td> üé® Joined <strong>Adobe Firefly</strong> team as Research Intern; exploring some cool stuff in generative models. üöÄ </td> </tr> <tr> <th scope="row">Mar 11, 2025</th> <td> üñºÔ∏è Joined <strong>SonyAI</strong> (Vision Foundation Model and Generative AI) as Research Intern; working on multimodal generative models. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/refedit.jpg"></div> <div id="pathiraja2025refedit" class="col-sm-8"> <div class="title">RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model for Referring Expression</div> <div class="author"> Bimsara Pathiraja *,¬†Maitreya Patel *,¬†Shivam Singh,¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a>,¬†and¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In ICCV</em></b> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2506.03448" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/bimsarapathiraja/refedit" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://refedit.vercel.app/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://huggingface.co/spaces/FlowChef/RefEdit-SD3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pathiraja2025refedit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model for Referring Expression}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pathiraja *, Bimsara and Patel *, Maitreya and Singh, Shivam and Yang, Yezhou and Baral, Chitta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://refedit.vercel.app/}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/FlowChef/RefEdit-SD3}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/flowchef.gif"></div> <div id="patel2024flowchef" class="col-sm-8"> <div class="title">Steering Rectified Flow Models in the Vector Field for Controlled Image Generation</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Song Wen,¬†<a href="https://scholar.google.com/citations?user=a7VNhCIAAAAJ" style="color:black;" target="_blank" rel="noopener noreferrer">Dimitris N. Metaxas</a>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In ICCV</em></b> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2412.00100" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/FlowChef/flowchef" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://flowchef.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://huggingface.co/spaces/FlowChef/FlowChef-Flux1-dev" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2024flowchef</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Steering Rectified Flow Models in the Vector Field for Controlled Image Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Wen, Song and Metaxas, Dimitris N. and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://flowchef.github.io/}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/FlowChef/FlowChef-Flux1-dev}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/voila.png"></div> <div id="yilmaz2025voila" class="col-sm-8"> <div class="title">Voil√†: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning</div> <div class="author"> Nilay Yilmaz,¬†<em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Yiran Luo,¬†<a href="https://www.tejasgokhale.com" style="color:black;" target="_blank" rel="noopener noreferrer">Tejas Gokhale</a>,¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a>,¬†Suren Jayasuriya, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Yezhou Yang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In ICLR (Main Conference) ‚Äì</em></b> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=q5MUMlHxpd" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/nlylmz/Voila" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Multimodal Large Language Models (MLLMs) have become a powerful tool for integrating visual and textual information. Despite their exceptional performance on visual understanding benchmarks, measuring their ability to reason abstractly across multiple images remains a significant challenge. To address this, we introduce VOILA , a large-scale, open-ended, dynamic benchmark designed to evaluate MLLMs‚Äô perceptual understanding and abstract relational reasoning. VOILA employs an analogical mapping approach in the visual domain, requiring models to generate an image that completes an analogy between two given image pairs, reference and application, without relying on predefined choices. Our experiments demonstrate that VOILA presents MLLMs with demanding relational reasoning tasks. Through multi-step analysis, we reveal that current MLLMs struggle to comprehend inter-image relationships and exhibit limited capabilities in highlevel relational reasoning. Notably, we observe that performance improves when using least-to-most prompting strategies. Comprehensive evaluations on opensource models and GPT-4o show that while the MolmoE-8B model achieves a state-of-the-art performance of 34% and 19% at finding the text-based answer to the questions on easy and hard scenarios, human performance consistently remains significantly higher at 70% on both difficulty scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yilmaz2025voila</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Voil√†: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yilmaz, Nilay and Patel, Maitreya and Luo, Yiran and Gokhale, Tejas and Baral, Chitta and Jayasuriya, Suren and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR (Main Conference) --}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tripletclip.png"></div> <div id="patel2024tripletclip" class="col-sm-8"> <div class="title">TripletCLIP: Improving Compositional Reasoning of CLIP via Vision-Language Negatives</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Abhiram Kusumba,¬†Sheng Cheng,¬†<a href="changhoonkim.com" style="color:black;">Changhoon Kim</a>,¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In NeurIPS (Main Conference) ‚Äì </em></b> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/tripletclip/TripletCLIP/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://huggingface.co/TripletCLIP/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Data</a> <a href="https://tripletclip.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between text and visual modalities to learn representations. This makes the nature of the training data a significant factor in the efficacy of CLIP for downstream tasks. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating ‚Äúhard‚Äù negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over 9% on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our negative data generation strategy, data, and code will be open-sourced.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2024tripletclip</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TripletCLIP: Improving Compositional Reasoning of CLIP via Vision-Language Negatives}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Kusumba, Abhiram and Cheng, Sheng and Kim, Changhoon and Baral, Chitta and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS (Main Conference) -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://tripletclip.github.io/}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://huggingface.co/TripletCLIP/}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/lambda.jpeg"></div> <div id="patel2024lambda" class="col-sm-8"> <div class="title">Œª-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Sangmin Jung,¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"> <b style="color:crimson">Media Coverages:</b>¬† <em> <a href="https://x.com/_akhaliq/status/1755798954476806422?s=20" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">AK</i> </a> </em>¬†<em> <a href="https://www.marktechpost.com/2024/02/23/arizona-state-university-researchers-%CE%BB-eclipse-a-novel-diffusion-free-methodology-for-personalized-text-to-image-t2i-applications/" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">, MarkTechPost</i> </a> </em> </div> <br> <div class="periodical"> <b><em>In Transactions on Machine Learning Research (TMLR)</em></b> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.05195" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/eclipse-t2i/lambda-eclipse-inference" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://eclipse-t2i.github.io/Lambda-ECLIPSE/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://huggingface.co/spaces/ECLIPSE-Community/lambda-eclipse-personalized-t2i" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Despite the recent advances in personalized text-to-image (P-T2I) generative models, it remains challenging to perform finetuning-free multi-subject-driven T2I in a resource-efficient manner. Predominantly, contemporary approaches, involving the training of Hypernetworks and Multimodal Large Language Models (MLLMs), require heavy computing resources that range from 600 to 12300 GPU hours of training. In this paper, we present Œª-ECLIPSE, an alternative prior-training strategy that works in the latent space of a pre-trained CLIP model without relying on the diffusion UNet models. Œª-ECLIPSE leverages the novel image-text interleaved pre-training for fast and effective multi-subject-driven P-T2I. Through extensive experiments, we establish that Œª-ECLIPSE surpasses existing baselines in composition alignment while preserving concept alignment performance, even with significantly lower resource utilization. Œª-ECLIPSE performs multi-subject driven P-T2I with just 34M parameters and is trained on a mere 74 GPU hours. Additionally, Œª-ECLIPSE achieves better balance with Canny edge-controlled subject-driven generations and demonstrates the unique ability to perform multi-concept interpolations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2024lambda</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Œª-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Jung, Sangmin and Baral, Chitta and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://eclipse-t2i.github.io/Lambda-ECLIPSE/}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/ECLIPSE-Community/lambda-eclipse-personalized-t2i}</span><span class="p">,</span>
  <span class="na">media1</span> <span class="p">=</span> <span class="s">{https://x.com/_akhaliq/status/1755798954476806422?s=20}</span><span class="p">,</span>
  <span class="na">media2</span> <span class="p">=</span> <span class="s">{https://www.marktechpost.com/2024/02/23/arizona-state-university-researchers-%CE%BB-eclipse-a-novel-diffusion-free-methodology-for-personalized-text-to-image-t2i-applications/}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/sheng_emnlp.png"></div> <div id="cheng2024precision" class="col-sm-8"> <div class="title">Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model</div> <div class="author"> Sheng Cheng,¬†<em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In EMNLP (findings) ‚Äì </em></b> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/shengcheng/Captions4T2I" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Recent text-to-image models often struggle to generate images that accurately align with given text, due to misalignments between training image and text pairs. In this paper, we analyze the critical role of caption precision and recall in text-to-image model training. Our analysis of human-annotated captions shows that both precision and recall are important for text-image alignment, but precision has a more significant impact. Leveraging these insights, we utilize Large Vision Language Models to generate synthetic captions for training. The performance of these models, closely mirroring that of models trained using human-annotated captions, underscores insights for the future use of synthetic data in text-to-image training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cheng2024precision</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cheng, Sheng and Patel, Maitreya and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP (findings) -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/eclipse.png"></div> <div id="patel2024eclipse" class="col-sm-8"> <div class="title">ECLIPSE:A Resource-Efficient Text-to-Image Prior for Image Generations</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†<a href="changhoonkim.com" style="color:black;">Changhoon Kim</a>,¬†Sheng Cheng,¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"> <b style="color:crimson">Media Coverages:</b>¬† <em> <a href="https://x.com/_akhaliq/status/1734036192817971630?s=61&amp;t=IcCTFKB2lsUjjS6XkHQ9Tw" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">AK</i> </a> </em>¬†<em> <a href="https://www.marktechpost.com/2023/12/13/this-ai-research-from-arizona-state-university-unveil-eclipse-a-novel-contrastive-learning-strategy-to-improve-the-text-to-image-non-diffusion-prior/" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">, MarkTechPost</i> </a> </em><em> <a href="https://multiplatform.ai/eclipse-a-game-changer-in-text-to-image-generation-unveiled-by-arizona-state-university/" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">, MultiplatformAI</i> </a> </em><em> <a href="https://youtu.be/jjcMmIGottQ?si=DKqWE8lnOfjB-rx9" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">, Paper Digest</i> </a> </em> </div> <br> <div class="periodical"> <b><em>In CVPR ‚Äì </em></b> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2312.04655" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/eclipse-t2i/eclipse-inference" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://eclipse-t2i.vercel.app" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE, a novel contrastive learning method that is both parameter and data-efficient. ECLIPSE¬†leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE¬†trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA larger models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE¬†consistently delivers high performance while significantly reducing resource dependency.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2024eclipse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ECLIPSE:A Resource-Efficient Text-to-Image Prior for Image Generations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Kim, Changhoon and Cheng, Sheng and Baral, Chitta and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://eclipse-t2i.vercel.app}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2}</span><span class="p">,</span>
  <span class="na">media1</span> <span class="p">=</span> <span class="s">{https://x.com/_akhaliq/status/1734036192817971630?s=61&amp;t=IcCTFKB2lsUjjS6XkHQ9Tw}</span><span class="p">,</span>
  <span class="na">media2</span> <span class="p">=</span> <span class="s">{https://www.marktechpost.com/2023/12/13/this-ai-research-from-arizona-state-university-unveil-eclipse-a-novel-contrastive-learning-strategy-to-improve-the-text-to-image-non-diffusion-prior/}</span><span class="p">,</span>
  <span class="na">media3</span> <span class="p">=</span> <span class="s">{https://multiplatform.ai/eclipse-a-game-changer-in-text-to-image-generation-unveiled-by-arizona-state-university/}</span><span class="p">,</span>
  <span class="na">media4</span> <span class="p">=</span> <span class="s">{https://youtu.be/jjcMmIGottQ?si=DKqWE8lnOfjB-rx9}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/wouaf_model.png"></div> <div id="kim2024wouaf" class="col-sm-8"> <div class="title">WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models</div> <div class="author"> <a href="changhoonkim.com" style="color:black;">Changhoon Kim*</a>,¬†<a href="https://sites.google.com/view/kylemin" style="color:black;" target="_blank" rel="noopener noreferrer">Kyle Min*</a>,¬†<em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Sheng Cheng,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"> <b style="color:crimson">Media Coverages:</b>¬† <em> <a href="https://x.com/_akhaliq/status/1683678703048613888?s=46&amp;t=XjL7XPmZ27Ev6ec3m2Wopw" target="_blank" rel="noopener noreferrer"> <i style="color:#00369f;">AK</i> </a> </em>¬†</div> <br> <div class="periodical"> <b><em>In CVPR ‚Äì </em></b> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2306.04744" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://wouaf.vercel.app/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://wouaf.vercel.app/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://huggingface.co/spaces/mpatel57/WOUAF-Text-to-Image" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>The rapid advancement of generative models, facilitating the creation of hyper-realistic images from textual descriptions, has concurrently escalated critical societal concerns such as misinformation. Traditional fake detection mechanisms, although providing some mitigation, fall short in attributing responsibility for the malicious use of synthetic images. This paper introduces a novel approach to model fingerprinting that assigns responsibility for the generated images, thereby serving as a potential countermeasure to model misuse. Our method modifies generative models based on each user‚Äôs unique digital fingerprint, imprinting a unique identifier onto the resultant content that can be traced back to the user. This approach, incorporating fine-tuning into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates near-perfect attribution accuracy with a minimal impact on output quality. We rigorously scrutinize our method‚Äôs secrecy under two distinct scenarios: one where a malicious user attempts to detect the fingerprint, and another where a user possesses a comprehensive understanding of our method. We also evaluate the robustness of our approach against various image post-processing manipulations typically executed by end-users. Through extensive evaluation of the Stable Diffusion models, our method presents a promising and novel avenue for accountable model distribution and responsible use.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kim2024wouaf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim*, Changhoon and Min*, Kyle and Patel, Maitreya and Cheng, Sheng and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://wouaf.vercel.app/}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/mpatel57/WOUAF-Text-to-Image}</span><span class="p">,</span>
  <span class="na">media1</span> <span class="p">=</span> <span class="s">{https://x.com/_akhaliq/status/1683678703048613888?s=46&amp;t=XjL7XPmZ27Ev6ec3m2Wopw}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/conceptbed.png"></div> <div id="patel2023conceptbed" class="col-sm-8"> <div class="title">ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†<a href="https://www.tejasgokhale.com" style="color:black;" target="_blank" rel="noopener noreferrer">Tejas Gokhale</a>,¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In AAAI‚Äô24 | Diffusion Workshop at NeurIPS ‚Äì </em></b> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2306.04695" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/ConceptBed/evaluations/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://conceptbed.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> <a href="https://huggingface.co/spaces/mpatel57/ConceptBed" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either objects, attributes, or styles, and also evaluate four dimensions of compositionality: counting, attributes, relations, and actions. Our human study shows that CCD is highly correlated with human understanding of concepts. Our results point to a trade-off between learning the concepts and preserving the compositionality which existing approaches struggle to overcome.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2023conceptbed</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Gokhale, Tejas and Baral, Chitta and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI'24 | Diffusion Workshop at NeurIPS -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://conceptbed.github.io/}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://huggingface.co/spaces/mpatel57/ConceptBed}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/cripp_example.gif"></div> <div id="patel2022cripp" class="col-sm-8"> <div class="title">CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†<a href="https://www.tejasgokhale.com" style="color:black;" target="_blank" rel="noopener noreferrer">Tejas Gokhale</a>,¬†<a href="https://www.public.asu.edu/~cbaral/" style="color:black;" target="_blank" rel="noopener noreferrer">Chitta Baral</a>,¬†and¬†<a href="https://yezhouyang.engineering.asu.edu" style="color:black;" target="_blank" rel="noopener noreferrer">Yezhou Yang</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In EMNLP, Main Conference ‚Äì </em></b> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2211.03779" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Maitreyapatel/CRIPP-VQA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://maitreyapatel.com/CRIPP-VQA/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Videos often capture objects, their motion, and the interactions between different objects. Although real-world objects have physical properties associated with them, many of these properties (such as mass and coefficient of friction) are not captured directly by the imaging pipeline. However, these properties can be estimated by utilizing cues from relative object motion and the dynamics introduced by collisions. In this paper, we introduce a new video question answering task for reasoning about the implicit physical properties of objects in a scene, from videos. For this task, we introduce a dataset ‚Äì CRIPP-VQA, which contains videos of objects in motion, annotated with hypothetical/counterfactual questions about the effect of actions (such as removing, adding, or replacing objects), questions about planning (choosing actions to perform in order to reach a particular goal), as well as descriptive questions about the visible properties of objects. We benchmark the performance of existing video question answering models on two test settings of CRIPP-VQA: \textiti.i.d. and an out-of-distribution setting which contains objects with values of mass, coefficient of friction, and initial velocities that are not seen in the training distribution. Our experiments reveal a surprising and significant performance gap in terms of answering questions about implicit properties (the focus of this paper) and explicit properties (the focus of prior work) of objects.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2022cripp</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CRIPP-VQA}: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Gokhale, Tejas and Baral, Chitta and Yang, Yezhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP, Main Conference -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://maitreyapatel.com/CRIPP-VQA/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/instruct.png"></div> <div id="wang2022benchmarking" class="col-sm-8"> <div class="title">Benchmarking generalization via in-context instructions on 1,600+ language tasks</div> <div class="author"> Yizhong Wang,¬†Swaroop Mishra,¬†Pegah Alipoormolabashi,¬†Yeganeh Kordi,¬†Amirreza Mirzaei,¬†and¬† others</div> <div class="media"></div> <br> <div class="periodical"> <b><em>In EMNLP, Main Conference ‚Äì </em></b> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.07705.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://instructions.apps.allenai.org" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce Natural-Instructions v2, a benchmark of 1,600+ diverse language tasks and their expert-written instructions. It covers 70+ distinct task types, such as tagging, in-filling, and rewriting. These tasks are collected with contributions of NLP practitioners in the community and through an iterative peer review process to ensure their quality. With this large and diverse collection of tasks, we are able to rigorously benchmark cross-task generalization of models ‚Äì training on a subset of tasks and evaluating on the remaining unseen ones. For instance, we quantify generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances, and model sizes. Based on these insights, we introduce Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) which outperforms existing larger models on our benchmark. We hope this benchmark facilitates future progress toward more general-purpose language understanding models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2022benchmarking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Benchmarking generalization via in-context instructions on 1,600+ language tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP, Main Conference -- }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://instructions.apps.allenai.org}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/mspecnet.png"></div> <div id="malaviya2020mspec" class="col-sm-8"> <div class="title">MSpeC-Net: Multi-Domain Speech Conversion Network</div> <div class="author"> Harshit Malaviya,¬†Jui Shah,¬†<em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Jalansh Munshi,¬†and¬†<a href="https://sites.google.com/site/hemantpatildaiict/" style="color:black;" target="_blank" rel="noopener noreferrer">Hemant A Patil</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In 45th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em></b> 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9052966" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Maitreyapatel/speech-conversion-between-different-modalities" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://maitreyapatel.github.io/mspec-net-demo/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">malaviya2020mspec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{MSpeC-Net}: Multi-Domain Speech Conversion Network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Malaviya, Harshit and Shah, Jui and Patel, Maitreya and Munshi, Jalansh and Patil, Hemant A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{45th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7764--7768}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/cinc.png"></div> <div id="patel2020cinc" class="col-sm-8"> <div class="title">CinC-GAN for Effective F0 prediction for Whisper-to-Normal Speech Conversion</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Mirali Purohit,¬†Jui Shah,¬†and¬†<a href="https://sites.google.com/site/hemantpatildaiict/" style="color:black;" target="_blank" rel="noopener noreferrer">Hemant A Patil</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In 28th European Signal Processing Conference (EUSIPCO)</em></b> 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2008.07788.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Maitreyapatel/speech-conversion-between-different-modalities" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2020cinc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CinC-GAN} for Effective F0 prediction for Whisper-to-Normal Speech Conversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Purohit, Mirali and Shah, Jui and Patil, Hemant A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{28th European Signal Processing Conference (EUSIPCO)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/weak_speech.png"></div> <div id="purohit2020weak" class="col-sm-8"> <div class="title">Weak Speech Supervision: A case study of Dysarthria Severity Classification</div> <div class="author"> Mirali Purohit,¬†<a href="https://scholar.google.com/citations?user=2UPwJC4AAAAJ&amp;hl=en" style="color:black;" target="_blank" rel="noopener noreferrer">Mihir Parmar</a>,¬†<em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†Harshit Malaviya,¬†and¬†<a href="https://sites.google.com/site/hemantpatildaiict/" style="color:black;" target="_blank" rel="noopener noreferrer">Hemant A Patil</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In 28th European Signal Processing Conference (EUSIPCO)</em></b> 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9287502" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">purohit2020weak</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Weak Speech Supervision: A case study of Dysarthria Severity Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Purohit, Mirali and Parmar, Mihir and Patel, Maitreya and Malaviya, Harshit and Patil, Hemant A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{28th European Signal Processing Conference (EUSIPCO)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/adagan.png"></div> <div id="patel2019novel" class="col-sm-8"> <div class="title">Novel adaptive generative adversarial network for voice conversion</div> <div class="author"> <em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†<a href="https://scholar.google.com/citations?user=2UPwJC4AAAAJ&amp;hl=en" style="color:black;" target="_blank" rel="noopener noreferrer">Mihir Parmar</a>,¬†Savan Doshi,¬†Nirmesh J Shah,¬†and¬†<a href="https://sites.google.com/site/hemantpatildaiict/" style="color:black;" target="_blank" rel="noopener noreferrer">Hemant A Patil</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In 11th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</em></b> 2019 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9023141" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">patel2019novel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Novel adaptive generative adversarial network for voice conversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel, Maitreya and Parmar, Mihir and Doshi, Savan and Shah, Nirmesh J and Patil, Hemant A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{11th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1273--1281}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> <li> <div class="row"> <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/first.png"></div> <div id="parmar2019effectiveness" class="col-sm-8"> <div class="title">Effectiveness of cross-domain architectures for whisper-to-normal speech conversion</div> <div class="author"> <a href="https://scholar.google.com/citations?user=2UPwJC4AAAAJ&amp;hl=en" style="color:black;" target="_blank" rel="noopener noreferrer">Mihir Parmar</a>,¬†Savan Doshi,¬†Nirmesh J Shah,¬†<em> <b><i style="color:#00369f;">Maitreya Patel</i></b> </em>,¬†and¬†<a href="https://sites.google.com/site/hemantpatildaiict/" style="color:black;" target="_blank" rel="noopener noreferrer">Hemant A Patil</a> </div> <div class="media"></div> <br> <div class="periodical"> <b><em>In 27th European Signal Processing Conference (EUSIPCO)</em></b> 2019 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8902961" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">parmar2019effectiveness</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Effectiveness of cross-domain architectures for whisper-to-normal speech conversion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parmar, Mihir and Doshi, Savan and Shah, Nirmesh J and Patel, Maitreya and Patil, Hemant A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{27th European Signal Processing Conference (EUSIPCO)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <hr> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Maitreya Patel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Modified by <a href="https://github.com/Maitreyapatel" target="_blank" rel="noopener noreferrer">Maitreya Patel</a>. Last updated: September 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-MYMZJNRLP7"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MYMZJNRLP7");</script> </body> </html>