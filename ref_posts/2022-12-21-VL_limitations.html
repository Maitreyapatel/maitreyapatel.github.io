<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>What is going on with multi-modal representation learning? | Maitreya Patel</title> <meta name="author" content="Maitreya Patel"/> <meta name="description" content="Reviewing limitations of existing VL research."/> <meta name="keywords" content="Maitreya, GenerativeAI, Multimodal, Diffusion/Flow Modeling"/> <meta property="og:site_name" content="Maitreya Patel"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Maitreya Patel | What is going on with multi-modal representation learning?"/> <meta property="og:url" content="https://maitreyapatel.com/ref_posts/2022-12-21-VL_limitations.html"/> <meta property="og:description" content="Reviewing limitations of existing VL research."/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="What is going on with multi-modal representation learning?"/> <meta name="twitter:description" content="Reviewing limitations of existing VL research."/> <meta name="twitter:site" content="@patelmaitreya"/> <meta name="twitter:creator" content="@patelmaitreya"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maitreyapatel.com/ref_posts/2022-12-21-VL_limitations.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "What is going on with multi-modal representation learning?",
      "description": "Reviewing limitations of existing VL research.",
      "published": "December 21, 2022",
      "authors": [
        {
          "author": "Maitreya Patel",
          "authorURL": "maitreyapatel.com",
          "affiliations": [
            {
              "name": "ASU",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Maitreya </span>Patel</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">Community Services</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>What is going on with multi-modal representation learning?</h1> <p>Reviewing limitations of existing VL research.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#pre-training-of-llms-vs-multi-modal-models">Pre-training of LLMs vs. Multi-modal models</a></div> <div><a href="#limitations-of-these-strategies">Limitations of these strategies</a></div> <div><a href="#possible-solutions">Possible Solutions</a></div> <div><a href="#relative-pre-training">Relative pre-training?</a></div> <div><a href="#data-augmentations">Data Augmentations</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>The multi-modal system refers to the idea of combining the vision+text+speech. More specifically, Vision-Language (VL) is becoming more accurate and accessible. Especially, Text-to-Image (T2I) diffusion models were the key highlight of 2022. Apart from T2I other VL tasks such as Captioning, Question-Answering, etc. are equally important and the latest proposed methodologies keep pushing the performance with the use of external data (LAION-2B), different pre-training strategies (SimVLN, BLIPs, etc.), increasing the computational complexity, and removing various spurious biases via either data augmentation or casual interventions.</p> <p>Although T2I results are quite a breakthrough and show huge potential, Visual Grounding and Visual Reasoning methods seem to be saturated in terms of performance improvements. Recent various benchmark studies (Winoground etc.) the existing models’ ability to understand compositionality. For example, <strong>“[grass] in a [mug]”</strong> is <code class="language-plaintext highlighter-rouge">semantically different</code> than <strong>“[mug] in a [grass]”</strong>. These experiments show that current approaches struggle to accurately distinguish these semantic perturbations, which suggests that current approaches are a <code class="language-plaintext highlighter-rouge">glorified</code> and <code class="language-plaintext highlighter-rouge">advanced</code> version of learning the correlations between visually seen and textually present entities.</p> <p><strong>Let’s achieve the 100% accuracy on winoground with this article.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/9-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/9-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/9-1400.webp"></source> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"></source> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> show the examples of winoground or other interesting dataset </div> <p>Since the raise of GPT3 and prompt/instruction learning, everyone knows the potential and impact of these systems in day-to-day life. It has been shown that LLMs contain a vast amount of commonsense knowledge and they do not require additional fine-tuning to have a decent performance. However, it seems that research in VL is stuck in learning the correlations. Even if these methods can overcome the challenge of compositionality, can they do commonsense reasoning?</p> <p>There are several future directions to can be taken from this point onwards to build a true Artificial General Intelligence (AGI). Jokes aside, we do not know what AGI even means but it would be sufficient if we can even develop Human-Level AI (HLAI), which can learn to reason about commonsense in the real-world and overcome the compositionality.</p> <blockquote> <p><strong>Direction 1:</strong> Good news! We have LLMs with good reasoning capabilities. If we can design a system that can effectively ground the videos/images for LLMs to do the reasoning. Or maybe let LLMs control the VL models to get more accurate information from visuals.</p> </blockquote> <p>The above direction is really simplistic and might work for a short period of time but still, this is not sufficient. <code class="language-plaintext highlighter-rouge">Spoiler alert:</code> Captions are not sufficient enough to develop a valid context for LLMs. Theoretically, each image has infinite observations and possibilities, which is hard to impossible to iterate through efficiently. Therefore, we need to create the systems following direction 2:</p> <blockquote> <p><strong>Direction 2:</strong> Answer is pretty straight forward!! We need to design multi-modal systems which have the same capabilities as LLMs (like GPT3). Initial work (such as Flamingo) has shown its potential for this. But we are just at the beginning phase and there is so much to be done. <code class="language-plaintext highlighter-rouge">Can concepts from causality be the key to achieving this?</code></p> </blockquote> <p>With this article, the authors would dive deep into the technical details to validate the above-proposed hypothesis. This study assumes to have some amount of prior knowledge about Vision-Language systems and it is divided into several sections. <strong>Section 2</strong> summarizes the existing VL pre-training strategies, <strong>Section 3</strong> compares the pre-training strategies of LLMs vs. VL models, <strong>Section 4</strong> shows the limitations of these methods using existing benchmark results, <strong>Section 5</strong> proposes the quick and easy solutions to overcome these limitations to show the future potential directions, and <strong>Section 6</strong> discusses various questions which need to be (or hopefully will be) answered soon.</p> <p>Reach out to the authors for feedback or questions!</p> <h2 id="pre-training-of-llms-vs-multi-modal-models">Pre-training of LLMs vs. Multi-modal models</h2> <p>Wheather it is LLMs or Multi-modal models, different pre-training strategies have shown the impact. These pre-training methods can be broadly classified into two broad categories: 1) Discriminative, and 2) Generative.</p> <p>Discriminative strategies involve various sub pre-trainings such as next sentence prediction or image-text alignment, and masked token (either image patch or word token) predictions. Initial results on BERT/RoBERTa, VinVL/ViLT etc. have shown the great results on down-stream tasks.</p> <p>Generative modeling strategies are getting more and more popular recently. We all know about GPT3 and how it impacted the reseach coomunity. Because of the generative pre-training, we have seen many immergent abilities like prompt/instrunction learning. Similarly, VL models are also being adapated to generative modeling and they have shown quite a promising results.</p> <h2 id="limitations-of-these-strategies">Limitations of these strategies</h2> <p>Inspiring from Language-Modeling, existing SotA VL models are all about increasing the model complexity and adding more data. No doubt this will improve the results on existing benchmarks and that’s impressive for sure. But the question still remains that do these models have the semantic understandings or not? With the recent benchmarks (as listed below) it can been seen that these models performs very poorly and suggests that they are just learning correlation.</p> <ol> <li>Winoground – Compositional Image-Text Alignment</li> <li>VALSE – Compositional semantic understanding on diverse categories</li> <li>VQA-LOL – Logical Reasoning</li> <li>Crepé – Compositional Hard-Negative sampling based Image/Text Retrieval</li> <li>Visor – Relative Spatial Reasoning</li> </ol> <h3 id="results-on-winogrond">Results on winogrond:</h3> <p>As shown in Fig. (1) the winoground focuses on learning semantic understanding from the images. By just swapping the two words in text the meaning completely changes. And VL models fail to detect these semantic perturbations. From Table, we can observe that all SotA VL models performs close to random.</p> <div class="row"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"></source> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> winoground table </div> <h3 id="results-on-crepé">Results on Crepé:</h3> <p>Similarly, Crepé analyses the compositionality at different levels of difficulty. And again the results are really poor.</p> <div class="row"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"></source> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Crepé table </div> <h2 id="possible-solutions">Possible Solutions</h2> <p>After just glancing through the challenges of VL models it becomes obvious that we need to answer the following questions:</p> <ol> <li>How can we avoid the spurious correlations being learned by the model?</li> <li>Can these models have commonsense reasoning abilities?</li> <li>Are existing pre-training sufficient enough to get the breakthrough?</li> </ol> <h3 id="how-can-we-avoid-the-spurious-correlations-being-learned-by-the-model">How can we avoid the spurious correlations being learned by the model?</h3> <p>This is really broad question to ask. And this issue is observed in almost all machine learning problems. Community has started adapting the concepts for causality to minimize learning the spurious correlations. For example, VL-Bias attempts to remove the language bias by performing interventions, which resulted in model being robust to some spurious correlations (For example, model stopped predicted all bananas being yellow to actually observing the image and answer the correct color of the bananas). Similarly, Visual-Commonsense RCNN attempt to remove the object-object relation based spurious correlation for object-detection.</p> <p>Therefore, we strongly believe that causal interventions are the way to move forward.</p> <h3 id="can-these-models-have-commonsense-reasoning-abilities">Can these models have commonsense reasoning abilities?</h3> <p>We have seen that LLMs have impressive commonsense reasoning skills. While, VL models seem to struggle a lot. For example, to reason about the consequences of kicking a football vs. brick model first needs to understand the visual aspects correctly (where the first problem becomes quite relevant) and then do the reasoning about the consequences.</p> <h3 id="are-existing-pre-training-sufficient-enough-to-get-the-breakthrough">Are existing pre-training sufficient enough to get the breakthrough?</h3> <p>After looking into previous two questions, we know that next goals to achieve. <strong>But HOW??</strong></p> <p>To answer this question, first we need to forget about the adding the data or increasing the models’ computational complexity. To make the significant improvement, we need to design the better pre-training strategies and once we achieve the promising results on smaller scale, adding the more resources will further boost the performance.</p> <p>Now, that’s cleared up, what are the best ways to improve the pre-training strategies?</p> <p>Well, we can’t answer that :/ This is the research problem that we need to work on. But in this article, we propose potential immediate next steps which shows the promising results. <code class="language-plaintext highlighter-rouge">Spolier:</code> We can get achieve 100% ACCURACY on winoground benchmark without task specific fine-tuning. And our answer is <strong>curriculum learning</strong>.</p> <p>As we previosuly discussed that VL models relys on the Image-Text Alignment and Mask Token Predictions. However, the Image-Text Alignment is done at random with 50% being the correct pair and another 50% either image or text is randomly selected from the corpus. Let’s understand the issue in this pre-training by considering an intuitive example. Suppose, we have an ground truth image-text pair (<code class="language-plaintext highlighter-rouge">a mug in a grass</code>). Now, we create the another example where we change the text randomly (<code class="language-plaintext highlighter-rouge">a horse is running on the seashore</code>)) and keep the same image. As we can see that by selecting the random text for creating negative example is really easy for models to overfit. To learn the accurate semantic meaning models need to learning to differentiate the semantically similar examples. Contrastive learning strategies are great but if we look at mathamatically what are the chances of getting a positive &amp; negative example pairs which are semantically close to each other? Yesss, you are thinking it correctly! It’s close to <strong>zero</strong>!!!!!!!</p> <p>Okay, so obvious solution is to just train the model on semantically similar data and boom you are good to go. But sadly that’s not the way to move forward. Because of the computational compexity of the recent models it becomes really difficult to train the models hard examples and expect it to achieve the SotA performance. Research in LLMs has shown that training these huge models itself is challenging. And therefore, we need to design a curriculum learning strategies where we increase the difficulty of the dataset slowly and let the model converge.</p> <p>Now, let’s see how to achieve the SotA performance on winoground and crepé without any prior knoweldge or even fine-tuning.</p> <h4 id="maybe-we-can-do-small-scale-experiment-and-put-some-results-here">Maybe we can do small scale experiment and put some results here.</h4> <h2 id="impact-of-constrastive-vs-comparitive-pre-training">Impact of Constrastive vs. Comparitive pre-training</h2> <p>Contrastive learning is well research training strategies. Is it really possible to improve it? If so how?</p> <p>In Computer Vision, Contrastive Learning is used as self-supervised training method and to be honest results are impressive. Here, we augment the input images using statistical methods and use these augmented examples for pre-training the model. However, to learning the semantic meaning within the images, maybe we can do the semantic image perturbation (i.e., Data Augmentations). However, this process is really expenstive in terms of resources and time (more about this in next section). Maybe we need something different here.</p> <p>How can we do the comparative pre-training? Let’s take the inspiration from the one of the closest problem that is <code class="language-plaintext highlighter-rouge">NLVR2</code>. It contains the problem statement, where model needs to compare two input images with respect to the given input text. And the way this dataset is designed is really useful. It contains the two images which are really close to each other in terms of semantics and needs to reason about the similarity and dissimilarity between them. So, thw question is <code class="language-plaintext highlighter-rouge">what if we train the VL model on such comparative setting?</code> To understand the impact of these, we take various fine-tuned models on NLVR2 and evaluate their performance on Winoground and Crepé. Results shows that these models perform &gt;90% on these benchmarks without any task related fine-tuning.</p> <p>Although this results are awesome and quite promising. These models have very low confidance at the same time. So there are still room for improvements but these pre-training strategy seems to be quite useful.</p> <h2 id="data-augmentations">Data Augmentations</h2> <p>TBD</p> <h2 id="conclusion">Conclusion</h2> <p>TBD</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Maitreya Patel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Modified by <a href="https://github.com/Maitreyapatel" target="_blank" rel="noopener noreferrer">Maitreya Patel</a>. Last updated: September 21, 2025. </div> </footer> <d-bibliography src="/assets/bibliography/"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-MYMZJNRLP7"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MYMZJNRLP7");</script> </body> </html>